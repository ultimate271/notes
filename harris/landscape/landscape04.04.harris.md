# Chapter 3 - The Tides of Bias
[Metadata]: # {04.04}
[Descriptor]: # {04.04}
[Author]: # {harris}
Chapter 3
The Tides of Bias
# The Tides of Bias
If one wants to understand how another person thinks, it is rarely sufficient
to know whether or not he believes a specific set of propositions. Two people
can hold the same belief for very different reasons, and such differences
generally matter. In the year 2003, it was one thing to believe that the United
States should not invade Iraq because the ongoing war in Afghanistan was more
important; it was another to believe it because you think it is an abomination
for infidels to trespass on Muslim land. Knowing what a person believes on a
specific subject is not identical to knowing how that person thinks.

Decades of psychological research suggest that unconscious processes influence
belief formation, and not all of them assist us in our search for truth. When
asked to judge the probability that an event will occur, or the likelihood that
one event caused another, people are frequently misled by a variety of factors,
including the unconscious influence of extraneous information. For instance, if
asked to recall the last four digits of their Social Security numbers and then
asked to estimate the number of doctors practicing in San Francisco, the
resulting numbers will show a statistically significant relationship. Needless
to say, when the order of questions is reversed, this effect disappears.36
There have been a few efforts to put a brave face on such departures from
rationality, construing them as random performance errors or as a sign that
experimental subjects have misunderstood the tasks presented to them—or even as
proof that research psychologists themselves have been beguiled by false norms
of reasoning. But efforts to exonerate our mental limitations have generally
failed. There are some things that we are just naturally bad at. And the
mistakes people tend to make across a wide range of reasoning tasks are not
mere errors; they are systematic errors that are strongly associated both
within and across tasks. As one might expect, many of these errors decrease as
cognitive ability increases.37 We also know that training, using both examples
and formal rules, mitigates many of these problems and can improve a person’s
thinking.38

Reasoning errors aside, we know that people often acquire their beliefs about
the world for reasons that are more emotional and social than strictly
cognitive. Wishful thinking, self-serving bias, in-group loyalties, and frank
self-deception can lead to monstrous departures from the norms of rationality.
Most beliefs are evaluated against a background of other beliefs and often in
the context of an ideology that a person shares with others. Consequently,
people are rarely as open to revising their views as reason would seem to
dictate.

On this front, the internet has simultaneously enabled two opposing influences
on belief: On the one hand, it has reduced intellectual isolation by making it
more difficult for people to remain ignorant of the diversity of opinion on any
given subject. But it has also allowed bad ideas to flourish—as anyone with a
computer and too much time on his hands can broadcast his point of view and,
often enough, find an audience. So while knowledge is increasingly open-source,
ignorance is, too.

It is also true that the less competent a person is in a given domain, the more
he will tend to overestimate his abilities. This often produces an ugly
marriage of confidence and ignorance that is very difficult to correct for.39
Conversely, those who are more knowledgeable about a subject tend to be acutely
aware of the greater expertise of others. This creates a rather unlovely
asymmetry in public discourse—one that is generally on display whenever
scientists debate religious apologists. For instance, when a scientist speaks
with appropriate circumspection about controversies in his field, or about the
limits of his own understanding, his opponent will often make wildly
unjustified assertions about just which religious doctrines can be inserted
into the space provided. Thus, one often finds people with no scientific
training speaking with apparent certainty about the theological implications of
quantum mechanics, cosmology, or molecular biology.

This point merits a brief aside: while it is a standard rhetorical move in such
debates to accuse scientists of being “arrogant,” the level of humility in
scientific discourse is, in fact, one of its most striking characteristics. In
my experience, arrogance is about as common at a scientific conference as
nudity. At any scientific meeting you will find presenter after presenter
couching his or her remarks with caveats and apologies. When asked to comment
on something that lies to either side of the very knife edge of their special
expertise, even Nobel laureates will say things like, “Well, this isn’t really
my area, but I would suspect that X is …” or “I’m sure there a several people
in this room who know more about this than I do, but as far as I know, X is …”
The totality of scientific knowledge now doubles every few years. Given how
much there is to know, all scientists live with the constant awareness that
whenever they open their mouths in the presence of other scientists, they are
guaranteed to be speaking to someone who knows more about a specific topic than
they do.


Cognitive biases cannot help but influence our public discourse. Consider
political conservatism: this is a fairly well-defined perspective that is
characterized by a general discomfort with societal change and a ready
acceptance of social inequality. As simple as political conservatism is to
describe, we know that it is governed by many factors. The psychologist John
Jost and colleagues analyzed data from twelve countries, acquired from 23,000
subjects, and found this attitude to be correlated with dogmatism,
inflexibility, death anxiety, need for closure, and anticorrelated with
openness to experience, cognitive complexity, self-esteem, and social
stability.40 Even the manipulation of a single of these variables can affect
political opinions and behavior. For instance, merely reminding people of the
fact of death increases their inclination to punish transgressors and to reward
those who uphold cultural norms. One experiment showed that judges could be led
to impose especially harsh penalties on prostitutes if they were simply
prompted to think about death prior to their deliberations.41

And yet after reviewing the literature linking political conservatism to many
obvious sources of bias, Jost and his coauthors reach the following conclusion:



Conservative ideologies, like virtually all other belief systems, are adopted
in part because they satisfy various psychological needs. To say that
ideological belief systems have a strong motivational basis is not to say that
they are unprincipled, unwarranted, or unresponsive to reason and evidence.42



This has more than a whiff of euphemism about it. Surely we can say that a
belief system known to be especially beholden to dogmatism, inflexibility,
death anxiety, and a need for closure will be less principled, less warranted,
and less responsive to reason and evidence than it would otherwise be.

This is not to say that liberalism isn’t also occluded by certain biases. In a
recent study of moral reasoning,43 subjects were asked to judge whether it was
morally correct to sacrifice the life of one person to save one hundred, while
being given subtle clues as to the races of the people involved. Conservatives
proved less biased by race than liberals and, therefore, more even-handed.
Liberals, as it turns out, were very eager to sacrifice a white person to save
one hundred nonwhites, but not the other way around—all the while maintaining
that considerations of race had not entered into their thinking. The point, of
course, is that science increasingly allows us to identify aspects of our minds
that cause us to deviate from norms of factual and moral reasoning—norms which,
when made explicit, are generally acknowledged to be valid by all parties.


There is a sense in which all cognition can be said to be motivated: one is
motivated to understand the world, to be in touch with reality, to remove
doubt, etc. Alternately, one might say that motivation is an aspect of
cognition itself.44 Nevertheless, motives like wanting to find the truth, not
wanting to be mistaken, etc., tend to align with epistemic goals in a way that
many other commitments do not. As we have begun to see, all reasoning may be
inextricable from emotion. But if a person’s primary motivation in holding a
belief is to hew to a positive state of mind—to mitigate feelings of anxiety,
embarrassment, or guilt, for instance—this is precisely what we mean by phrases
like “wishful thinking” and “self-deception.” Such a person will, of necessity,
be less responsive to valid chains of evidence and argument that run counter to
the beliefs he is seeking to maintain. To point out nonepistemic motives in
another’s view of the world, therefore, is always a criticism, as it serves to
cast doubt upon a person’s connection to the world as it is.45

