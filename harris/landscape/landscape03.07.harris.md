# Chapter 2 - The Moral Brain
[Metadata]: # {03.07}
[Descriptor]: # {03.07}
[Author]: # {harris}
Chapter 2
The Moral Brain
# The Moral Brain
Imagine that you are having dinner in a restaurant and spot your best friend’s
wife seated some distance away. As you stand to say hello, you notice that the
man seated across from her is not your best friend, but a handsome stranger.
You hesitate. Is he a colleague of hers from work? Her brother from out of
town? Something about the scene strikes you as illicit. While you cannot hear
what they are saying, there is an unmistakable sexual chemistry between them.
You now recall that your best friend is away at a conference. Is his wife
having an affair? What should you do?

Several regions of the brain will contribute to this impression of moral
salience and to the subsequent stirrings of moral emotion. There are many
separate strands of cognition and feeling that intersect here: sensitivity to
context, reasoning about other people’s beliefs, the interpretation of facial
expressions and body language, suspicion, indignation, impulse control, etc. At
what point do these disparate processes constitute an instance of moral
cognition? It is difficult to say. At a minimum, we know that we have entered
moral territory once thoughts about morally relevant events (e.g., the
possibility of a friend’s betrayal) have been consciously entertained. For the
purposes of this discussion, we need draw the line no more precisely than this.

The brain regions involved in moral cognition span many areas of the prefrontal
cortex and the temporal lobes. The neuroscientists Jorge Moll, Ricardo de
Oliveira-Souza, and colleagues have written the most comprehensive reviews of
this research.61 They divide human actions into four categories:



1. Self-serving actions that do not affect others



2. Self-serving actions that negatively affect others



3. Actions that are beneficial to others, with a high probability of
reciprocation (“reciprocal altruism”)



4. Actions that are beneficial to others, with no direct personal benefits
(material or reputation gains) and no expected reciprocation (“genuine
altruism”). This includes altruistic helping as well as costly punishment of
norm violators (“altruistic punishment”)62


As Moll and colleagues point out, we share behaviors 1 through 3 with other
social mammals, while 4 seems to be the special province of human beings. (We
should probably add that this altruism must be intentional/conscious, so as to
exclude the truly heroic self-sacrifice seen among eusocial insects like bees,
ants, and termites.) While Moll et al. admit to ignoring the reward component
of genuine altruism (often called the “warm glow” associated with cooperation),
we know from neuroimaging studies that cooperation is associated with
heightened activity in the brain’s reward regions.63 Here, once again, the
traditional opposition between selfish and selfless motivation seems to break
down. If helping others can be rewarding, rather than merely painful, it should
be thought of as serving the self in another mode.

It is easy to see the role that negative and positive motivations play in the
moral domain: we feel contempt/anger for the moral transgressions of others,
guilt/shame over our own moral failings, and the warm glow of reward when we
find ourselves playing nicely with other people. Without the engagement of such
motivational mechanisms, moral prescriptions (purely rational notions of
“ought”) would be very unlikely to translate into actual behaviors. The fact
that motivation is a separate variable explains the conundrum briefly touched
on above: we often know what would make us happy, or what would make the world
a better place, and yet we find that we are not motivated to seek these ends;
conversely, we are often motivated to behave in ways that we know we will later
regret. Clearly, moral motivation can be uncoupled from the fruits of moral
reasoning. A science of morality would, of necessity, require a deeper
understanding of human motivation.

The regions of the brain that govern judgments of right and wrong include a
broad network of cortical and subcortical structures. The contribution of these
areas to moral thought and behavior differs with respect to emotional tone:
lateral regions of the frontal lobes seem to govern the indignation associated
with punishing transgressors, while medial frontal regions produce the feelings
of reward associated with trust and reciprocation.64 As we will see, there is
also a distinction between personal and impersonal moral decisions. The
resulting picture is complicated: factors like moral sensitivity, moral
motivation, moral judgment, and moral reasoning rely on separable, mutually
overlapping processes.

The medial prefrontal cortex (MPFC) is central to most discussions of morality
and the brain. As discussed further in chapters 3 and 4, this region is
involved in emotion, reward, and judgments of self-relevance. It also seems to
register the difference between belief and disbelief. Injuries here have been
associated with a variety of deficits including poor impulse control, emotional
blunting, and the attenuation of social emotions like empathy, shame,
embarrassment, and guilt. When frontal damage is limited to the MPFC, reasoning
ability as well as the conceptual knowledge of moral norms are generally
spared, but the ability to behave appropriately toward others tends to be
disrupted.

Interestingly, patients suffering from MPFC damage are more inclined to
consequentialist reasoning than normal subjects are when evaluating certain
moral dilemmas—when, for instance, the means of sacrificing one person’s life
to save many others is personal rather than impersonal.65 Consider the
following two scenarios:



1. You are at the wheel of a runaway trolley quickly approaching a fork in the
tracks. On the tracks extending to the left is a group of five railway workmen.
On the tracks extending to the right is a single railway workman.



If you do nothing the trolley will proceed to the left, causing the deaths of
the five workmen. The only way to avoid the deaths of these workmen is to hit a
switch on your dashboard that will cause the trolley to proceed to the right,
causing the death of the single workman.



Is it appropriate for you to hit the switch in order to avoid the deaths of the
five workmen?



2. A runaway trolley is heading down the tracks toward five workmen who will be
killed if the trolley proceeds on its present course. You are on a footbridge
over the tracks, in between the approaching trolley and the five workmen. Next
to you on this footbridge is a stranger who happens to be very large.



The only way to save the lives of the five workmen is to push this stranger off
the bridge and onto the tracks below where his large body will stop the
trolley. The stranger will die if you do this, but the five workmen will be
saved.



Is it appropriate for you to push the stranger onto the tracks in order to save
the five workmen?66


Most people strongly support sacrificing one person to save five in the first
scenario, while considering such a sacrifice morally abhorrent in the second.
This paradox has been well known in philosophical circles for years.67 Joshua
Greene and colleagues were the first to look at the brain’s response to these
dilemmas using fMRI.68 They found that the personal forms of these dilemmas,
like the one described in scenario two, more strongly activate brain regions
associated with emotion. Another group has since found that the disparity
between people’s responses to the two scenarios can be modulated, however
slightly, by emotional context. Subjects who spent a few minutes watching a
pleasant video prior to confronting the footbridge dilemma were more apt to
push the man to his death.69

The fact that patients suffering from MPFC injuries find it easier to sacrifice
the one for the many is open to differing interpretations. Greene views this as
evidence that emotional and cognitive processes often work in opposition.70
There are reasons to worry, however, that mere opposition between
consequentialist thinking and negative emotion does not adequately account for
the data.71

I suspect that a more detailed understanding of the brain processes involved in
making moral judgments of this type could affect our sense of right and wrong.
And yet superficial differences between moral dilemmas may continue to play a
role in our reasoning. If losses will always cause more suffering than forsaken
gains, or if pushing a person to his death is guaranteed to traumatize us in a
way that throwing a switch will not, these distinctions become variables that
constrain how we can move across the moral landscape toward higher states of
well-being. It seems to me, however, that a science of morality can absorb
these details: scenarios that appear, on paper, to lead to the same outcome
(e.g., one life lost, five lives saved), may actually have different
consequences in the real world.

