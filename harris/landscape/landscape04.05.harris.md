# Chapter 3 - Mistaking Our Limits
[Metadata]: # {04.05}
[Descriptor]: # {04.05}
[Author]: # {harris}
Chapter 3
Mistaking Our Limits
# Mistaking Our Limits
We have long known, principally through the neurological work of Antonio
Damasio and colleagues, that certain types of reasoning are inseparable from
emotion.46 To reason effectively, we must have a feeling for the truth. Our
first fMRI study of belief and disbelief seemed to bear this out.47 If
believing a mathematical equation (vs. disbelieving another) and believing an
ethical proposition (vs. disbelieving another) produce the same changes in
neurophysiology, the boundary between scientific dispassion and judgments of
value becomes difficult to establish.

However, such findings do not in the least diminish the importance of reason,
nor do they blur the distinction between justified and unjustified belief. On
the contrary, the inseparability of reason and emotion confirms that the
validity of a belief cannot merely depend on the conviction felt by its
adherents; it rests on the chains of evidence and argument that link it to
reality. Feeling may be necessary to judge the truth, but it cannot be
sufficient.

The neurologist Robert Burton argues that the “feeling of knowing” (i.e., the
conviction that one’s judgment is correct) is a primary positive emotion that
often floats free of rational processes and can occasionally become wholly
detached from logical or sensory evidence.48 He infers this from neurological
disorders in which subjects display pathological certainty (e.g., schizophrenia
and Cotard’s delusion) and pathological uncertainty (e.g., obsessive-compulsive
disorder). Burton concludes that it is irrational to expect too much of human
rationality. On his account, rationality is mostly aspirational in character
and often little more than a façade masking pure, unprincipled feeling.

Other neuroscientists have made similar claims. Chris Frith, a pioneer in the
use of functional neuroimaging, recently wrote:



[W]here does conscious reasoning come into the picture? It is an attempt to
justify the choice after it has been made. And it is, after all, the only way
we have to try to explain to other people why we made a particular decision.
But given our lack of access to the brain processes involved, our justification
is often spurious: a post-hoc rationalization, or even a confabulation—a
“story” born of the confusion between imagination and memory.49



I doubt Frith meant to deny that reason ever plays a role in decision making
(though the title of his essay was “No One Really Uses Reason”). He has,
however, conflated two facts about the mind: while it is true that all
conscious processes, including any effort of reasoning, depend upon events of
which we are not conscious, this does not mean that reasoning amounts to little
more than a post hoc justification of brute sentiment. We are not aware of the
neurological processes that allow us to follow the rules of algebra, but this
doesn’t mean that we never follow these rules or that the role they play in our
mathematical calculations is generally post hoc. The fact that we are unaware
of most of what goes on in our brains does not render the distinction between
having good reasons for what one believes and having bad ones any less clear or
consequential. Nor does it suggest that internal consistency, openness to
information, self-criticism, and other cognitive virtues are less valuable than
we generally assume.

There are many ways to make too much of the unconscious underpinnings of human
thought. For instance, Burton observes that one’s thinking on many moral
issues—ranging from global warming to capital punishment—will be influenced by
one’s tolerance for risk. In evaluating the problem of global warming, one must
weigh the risk of melting the polar ice caps; in judging the ethics of capital
punishment, one must consider the risk of putting innocent people to death.
However, people differ significantly with respect to risk tolerance, and these
differences appear to be governed by a variety of genes—including genes for the
D4 dopamine receptor and the protein stathmin (which is primarily expressed in
the amygdala). Believing that there can be no optimal degree of risk aversion,
Burton concludes that we can never truly reason about such ethical questions.
“Reason” will simply be the name we give to our unconscious (and genetically
determined) biases. But is it really true to say that every degree of risk
tolerance will serve our purposes equally well as we struggle to build a global
civilization? Does Burton really mean to suggest that there is no basis for
distinguishing healthy from unhealthy—or even suicidal—attitudes toward risk?

As it turns out, dopamine receptor genes may play a role in religious belief as
well. People who have inherited the most active form of the D4 receptor are
more likely to believe in miracles and to be skeptical of science; the least
active forms correlate with “rational materialism.”50 Skeptics given the drug
L-dopa, which increases dopamine levels, show an increased propensity to accept
mystical explanations for novel phenomena.51 The fact that religious belief is
both a cultural universal and appears to be tethered to the genome has led
scientists like Burton to conclude that there is simply no getting rid of
faith-based thinking.

It seems to me that Burton and Frith have misunderstood the significance of
unconscious cognitive processes. On Burton’s account, worldviews will remain
idiosyncratic and incommensurable, and the hope that we might persuade one
another through rational argument and, thereby, fuse our cognitive horizons is
not only vain but symptomatic of the very unconscious processes and frank
irrationality that we would presume to expunge. This leads him to conclude that
any rational criticism of religious irrationality is an unseemly waste of time:



The science-religion controversy cannot go away; it is rooted in biology …
Scorpions sting. We talk of religion, afterlife, soul, higher powers, muses,
purpose, reason, objectivity, pointlessness, and randomness. We cannot help
ourselves … To insist that the secular and the scientific be universally
adopted flies in the face of what neuroscience tells us about different
personality traits generating idiosyncratic worldviews … Different genetics,
temperaments, and experience led to contrasting worldviews. Reason isn’t going
to bridge this gap between believers and nonbelievers.52



The problem, however, is that we could have said the same about witchcraft.
Historically, a preoccupation with witchcraft has been a cultural universal.
And yet belief in magic is now in disrepute almost everywhere in the developed
world. Is there a scientist on earth who would be tempted to argue that belief
in the evil eye or in the demonic origins of epilepsy is bound to remain
impervious to reason?

Lest the analogy between religion and witchcraft seem quaint, it is worth
remembering that belief in magic and demonic possession is still epidemic in
Africa. In Kenya elderly men and women are regularly burned alive as witches.53
In Angola, Congo, and Nigeria the hysteria has mostly targeted children:
thousands of unlucky boys and girls have been blinded, injected with battery
acid, and otherwise put to torture in an effort to purge them of demons; others
have been killed outright; many more have been disowned by their families and
rendered homeless.54 Needless to say, much of this lunacy has spread in the
name of Christianity. The problem is especially intractable because the
government officials charged with protecting these suspected witches also
believe in witchcraft. As was the case in the Middle Ages, when the belief in
witchcraft was omnipresent in Europe, only a truly panoramic ignorance about
the physical causes of disease, crop failure, and life’s other indignities
allows this delusion to thrive.

What if we were to connect the fear of witches with the expression of a certain
receptor subtype in the brain? Who would be tempted to say that the belief in
witchcraft is, therefore, ineradicable?

As someone who has received many thousands of letters and emails from people
who have ceased to believe in the God of Abraham, I know that pessimism about
the power of reason is unwarranted. People can be led to notice the
incongruities in their faith, the self-deception and wishful thinking of their
coreligionists, and the growing conflict between the claims of scripture and
the findings of modern science. Such reasoning can inspire them to question
their attachment to doctrines that, in the vast majority of cases, were simply
drummed into them on mother’s knee. The truth is that people can transcend mere
sentiment and clarify their thinking on almost any subject. Allowing competing
views to collide—through open debate, a willingness to receive criticism,
etc.—performs just such a function, often by exposing inconsistencies in a
belief system that make its adherents profoundly uncomfortable. There are
standards to guide us, even when opinions differ, and the violation of such
standards generally seems consequential to everyone involved.
Self-contradiction, for instance, is viewed as a problem no matter what one is
talking about. And anyone who considers it a virtue is very unlikely to be
taken seriously. Again, reason is not starkly opposed to feeling on this front;
it entails a feeling for the truth.

Conversely, there are occasions when a true proposition just doesn’t seem right
no matter how one squints one’s eyes or cocks one’s head, and yet its truth can
be acknowledged by anyone willing to do the necessary intellectual work. It is
very difficult to grasp that tiny quantities of matter contain vast amounts of
explosive energy, but the equations of physics—along with the destructive yield
of our nuclear bombs—confirms that this is so. Similarly, we know that most
people cannot produce or even recognize a series of digits or coin tosses that
meets a statistical test for randomness. But this has not stopped us from
understanding randomness mathematically—or from factoring our innate blindness
to randomness into our growing understanding of cognition and economic
behavior.55


The fact that reason must be rooted in our biology does not negate the
principles of reason. Wittgenstein once observed that the logic of our language
allows us to ask, “Was that gunfire?” but not “Was that a noise?”56 This seems
to be a contingent fact of neurology, rather than an absolute constraint upon
logic. A synesthete, for instance, who experiences crosstalk between his
primary senses (seeing sounds, tasting colors, etc.), might be able to pose the
latter question without any contradiction. How the world seems to us (and what
can be logically said about its seemings) depends upon facts about our brains.
Our inability to say that an object is “red and green all over” is a fact about
the biology of vision before it is a fact of logic. But that doesn’t prevent us
from seeing beyond this very contingency. As science advances, we are
increasingly coming to understand the natural limits of our understanding.

