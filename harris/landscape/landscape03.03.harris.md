# Chapter 2 - Moral Paradox
[Metadata]: # {03.03}
[Descriptor]: # {03.03}
[Author]: # {harris}
Chapter 2
Moral Paradox
# Moral Paradox
One of the problems with consequentialism in practice is that we cannot always
determine whether the effects of an action will be bad or good. In fact, it can
be surprisingly difficult to decide this even in retrospect. Dennett has dubbed
this problem “the Three Mile Island Effect.”22 Was the meltdown at Three Mile
Island a bad outcome or a good one? At first glance, it surely seems bad, but
it might have also put us on a path toward greater nuclear safety, thereby
saving many lives. Or it might have caused us to grow dependent on more
polluting technologies, contributing to higher rates of cancer and to global
climate change. Or it might have produced a multitude of effects, some mutually
reinforcing, and some mutually canceling. If we cannot determine the net result
of even such a well-analyzed event, how can we judge the likely consequences of
the countless decisions we must make throughout our lives?

One difficulty we face in determining the moral valence of an event is that it
often seems impossible to determine whose well-being should most concern us.
People have competing interests, mutually incompatible notions of happiness,
and there are many well-known paradoxes that leap into our path the moment we
begin thinking about the welfare of whole populations. As we are about to see,
population ethics is a notorious engine of paradox, and no one, to my
knowledge, has come up with a way of assessing collective well-being that
conserves all of our intuitions. As the philosopher Patricia Churchland puts
it, “no one has the slightest idea how to compare the mild headache of five
million against the broken legs of two, or the needs of one’s own two children
against the needs of a hundred unrelated brain-damaged children in Serbia.”23

Such puzzles may seem of mere academic interest, until we realize that
population ethics governs the most important decisions societies ever make.
What are our moral responsibilities in times of war, when diseases spread, when
millions suffer famine, or when global resources are scarce? These are moments
in which we have to assess changes in collective welfare in ways that purport
to be rational and ethical. Just how motivated should we be to act when 250,000
people die in an earthquake on the island of Haiti? Whether we know it or not,
intuitions about the welfare of whole populations determine our thinking on
these matters.

Except, that is, when we simply ignore population ethics—as, it seems, we are
psychologically disposed to do. The work of the psychologist Paul Slovic and
colleagues has uncovered some rather startling limitations on our capacity for
moral reasoning when thinking about large groups of people—or, indeed, about
groups larger than one.24 As Slovic observes, when human life is threatened, it
seems both rational and moral for our concern to increase with the number of
lives at stake. And if we think that losing many lives might have some
additional negative consequences (like the collapse of civilization), the curve
of our concern should grow steeper still. But this is not how we
characteristically respond to the suffering of other human beings.

Slovic’s experimental work suggests that we intuitively care most about a
single, identifiable human life, less about two, and we grow more callous as
the body count rises. Slovic believes that this “psychic numbing” explains the
widely lamented fact that we are generally more distressed by the suffering of
single child (or even a single animal) than by a proper genocide. What Slovic
has termed “genocide neglect”—our reliable failure to respond, both practically
and emotionally, to the most horrific instances of unnecessary human
suffering—represents one of the more perplexing and consequential failures of
our moral intuition.

Slovic found that when given a chance to donate money in support of needy
children, subjects give most generously and feel the greatest empathy when told
only about a single child’s suffering. When presented with two needy cases,
their compassion wanes. And this diabolical trend continues: the greater the
need, the less people are emotionally affected and the less they are inclined
to give.

Of course, charities have long understood that putting a face on the data will
connect their constituents to the reality of human suffering and increase
donations. Slovic’s work has confirmed this suspicion, which is now known as
the “identifiable victim effect.”25 Amazingly, however, adding information
about the scope of a problem to these personal appeals proves to be
counterproductive. Slovic has shown that setting the story of a single needy
person in the context of wider human need reliably diminishes altruism.

The fact that people seem to be reliably less concerned when faced with an
increase in human suffering represents an obvious violation of moral norms. The
important point, however, is that we immediately recognize how indefensible
this allocation of emotional and material resources is once it is brought to
our attention. What makes these experimental findings so striking is that they
are patently inconsistent: if you care about what happens to one little girl,
and you care about what happens to her brother, you must, at the very least,
care as much about their combined fate. Your concern should be (in some sense)
cumulative.26 When your violation of this principle is revealed, you will feel
that you have committed a moral error. This explains why results of this kind
can only be obtained between subjects (where one group is asked to donate to
help one child and another group is asked to support two); we can be sure that
if we presented both questions to each participant in the study, the effect
would disappear (unless subjects could be prevented from noticing when they
were violating the norms of moral reasoning).

Clearly, one of the great tasks of civilization is to create cultural
mechanisms that protect us from the moment-to-moment failures of our ethical
intuitions. We must build our better selves into our laws, tax codes, and
institutions. Knowing that we are generally incapable of valuing two children
more than either child alone, we must build a structure that reflects and
enforces our deeper understanding of human well-being. This is where a science
of morality could be indispensable to us: the more we understand the causes and
constituents of human fulfillment, and the more we know about the experiences
of our fellow human beings, the more we will be able to make intelligent
decisions about which social policies to adopt.

For instance, there are an estimated 90,000 people living on the streets of Los
Angeles. Why are they homeless? How many of these people are mentally ill? How
many are addicted to drugs or alcohol? How many have simply fallen through the
cracks in our economy? Such questions have answers. And each of these problems
admits of a range of responses, as well as false solutions and neglect. Are
there policies we could adopt that would make it easy for every person in the
United States to help alleviate the problem of homelessness in their own
communities? Is there some brilliant idea that no one has thought of that would
make people want to alleviate the problem of homelessness more than they want
to watch television or play video games? Would it be possible to design a video
game that could help solve the problem of homelessness in the real world?27
Again, such questions open onto a world of facts, whether or not we can bring
the relevant facts into view.

Clearly, morality is shaped by cultural norms to a great degree, and it can be
difficult to do what one believes to be right on one’s own. A friend’s
four-year-old daughter recently observed the role that social support plays in
making moral decisions:



“It’s so sad to eat baby lambies,” she said as she gnawed greedily on a lamb
chop.



“So, why don’t you stop eating them?” her father asked.



“Why would they kill such a soft animal? Why wouldn’t they kill some other kind
of animal?”



“Because,” her father said, “people like to eat the meat. Like you are, right
now.”



His daughter reflected for a moment—still chewing her lamb—and then replied:



“It’s not good. But I can’t stop eating them if they keeping killing them.”

And the practical difficulties for consequentialism do not end here. When
thinking about maximizing the well-being of a population, are we thinking in
terms of total or average well-being? The philosopher Derek Parfit has shown
that both bases of calculation lead to troubling paradoxes.28 If we are
concerned only about total welfare, we should prefer a world with hundreds of
billions of people whose lives are just barely worth living to a world in which
7 billion of us live in perfect ecstasy. This is the result of Parfit’s famous
argument known as “The Repugnant Conclusion.”29 If, on the other hand, we are
concerned about the average welfare of a population, we should prefer a world
containing a single, happy inhabitant to a world of billions who are only
slightly less happy; it would even suggest that we might want to painlessly
kill many of the least happy people currently alive, thereby increasing the
average of human well-being. Privileging average welfare would also lead us to
prefer a world in which billions live under the misery of constant torture to a
world in which only one person is tortured ever-so-slightly more. It could also
render the morality of an action dependent upon the experience of unaffected
people. As Parfit points out, if we care about the average over time, we might
deem it morally wrong to have a child today whose life, while eminently worth
living, would not compare favorably to the lives of the ancient Egyptians.
Parfit has even devised scenarios in which everyone alive could have a lower
quality of life than they otherwise would and yet the average quality of life
will have increased.30 Clearly, this proves that we cannot rely on a simple
summation or averaging of welfare as our only metric. And yet, at the extremes,
we can see that human welfare must aggregate in some way: it really is better
for all of us to be deeply fulfilled than it is for everyone to live in
absolute agony.

Placing only consequences in our moral balance also leads to indelicate
questions. For instance, do we have a moral obligation to come to the aid of
wealthy, healthy, and intelligent hostages before poor, sickly, and slow-witted
ones? After all, the former are more likely to make a positive contribution to
society upon their release. And what about remaining partial to one’s friends
and family? Is it wrong for me to save the life of my only child if, in the
process, I neglect to save a stranger’s brood of eight? Wrestling with such
questions has convinced many people that morality does not obey the simple laws
of arithmetic.

However, such puzzles merely suggest that certain moral questions could be
difficult or impossible to answer in practice; they do not suggest that
morality depends upon something other than the consequences of our actions and
intentions. This is a frequent source of confusion: consequentialism is less a
method of answering moral questions than it is a claim about the status of
moral truth. Our assessment of consequences in the moral domain must proceed as
it does in all others: under the shadow of uncertainty, guided by theory, data,
and honest conversation. The fact that it may often be difficult, or even
impossible, to know what the consequences of our thoughts and actions will be
does not mean that there is some other basis for human values that is worth
worrying about.


Such difficulties notwithstanding, it seems to me quite possible that we will
one day resolve moral questions that are often thought to be unanswerable. For
instance, we might agree that having a preference for one’s intimates is better
(in that it increases general welfare) than being fully disinterested as to how
consequences accrue. Which is to say that there may be some forms of love and
happiness that are best served by each of us being specially connected to a
subset of humanity. This certainly appears to be descriptively true of us at
present. Communal experiments that ignore parents’ special attachment to their
own children, for instance, do not seem to work very well. The Israeli
kibbutzim learned this the hard way: after discovering that raising children
communally made both parents and children less happy, they reinstated the
nuclear family.31 Most people may be happier in a world in which a natural bias
toward one’s own children is conserved—presumably in the context of laws and
social norms that disregard this bias. When I take my daughter to the hospital,
I am naturally more concerned about her than I am about the other children in
the lobby. I do not, however, expect the hospital staff to share my bias. In
fact, given time to reflect about it, I realize that I would not want them to.
How could such a denial of my self-interest actually be in the service of my
self-interest? Well, first, there are many more ways for a system to be biased
against me than in my favor, and I know that I will benefit from a fair system
far more than I will from one that can be easily corrupted. I also happen to
care about other people, and this experience of empathy deeply matters to me. I
feel better as a person valuing fairness, and I want my daughter to become a
person who shares this value. And how would I feel if the physician attending
my daughter actually shared my bias for her and viewed her as far more
important than the other patients under his care? Frankly, it would give me the
creeps.

But perhaps there are two possible worlds that maximize the well-being of their
inhabitants to precisely the same degree: in world X everyone is focused on the
welfare of all others without bias, while in world Y everyone shows some degree
of moral preference for their friends and family. Perhaps these worlds are
equally good, in that their inhabitants enjoy precisely the same level of
well-being. These could be thought of as two peaks on the moral landscape.
Perhaps there are others. Does this pose a threat to moral realism or to
consequentialism? No, because there would still be right and wrong ways to move
from our current position on the moral landscape toward one peak or the other,
and movement would still be a matter of increasing well-being in the end.

To bring the discussion back to the especially low-hanging fruit of
conservative Islam: there is absolutely no reason to think that demonizing
homosexuals, stoning adulterers, veiling women, soliciting the murder of
artists and intellectuals, and celebrating the exploits of suicide bombers will
move humanity toward a peak on the moral landscape. This is, I think, as
objective a claim as we ever make in science.

Consider the Danish cartoon controversy: an eruption of religious insanity that
still flows to this day. Kurt Westergaard, the cartoonist who drew what was
arguably the most inflammatory of these utterly benign cartoons has lived in
hiding since pious Muslims first began calling for his murder in 2006. A few
weeks ago—more than three years after the controversy first began—a Somali man
broke into Westergaard’s home with an axe. Only the construction of a specially
designed “safe room” allowed Westergaard to escape being slaughtered for the
glory of God (his five-year-old granddaughter also witnessed the attack).
Westergaard now lives with continuous police protection—as do the other
eighty-seven men in Denmark who have the misfortune of being named “Kurt
Westergaard.”32

The peculiar concerns of Islam have created communities in almost every society
on earth that grow so unhinged in the face of criticism that they will reliably
riot, burn embassies, and seek to kill peaceful people, over cartoons. This is
something they will not do, incidentally, in protest over the continuous
atrocities committed against them by their fellow Muslims. The reasons why such
a terrifying inversion of priorities does not tend to maximize human happiness
are susceptible to many levels of analysis—ranging from biochemistry to
economics. But do we need further information in this case? It seems to me that
we already know enough about the human condition to know that killing
cartoonists for blasphemy does not lead anywhere worth going on the moral
landscape.


There are other results in psychology and behavioral economics that make it
difficult to assess changes in human well-being. For instance, people tend to
consider losses to be far more significant than forsaken gains, even when the
net result is the same. For instance, when presented with a wager where they
stand a 50 percent chance of losing $100, most people will consider anything
less than a potential gain of $200 to be unattractive. This bias relates to
what has come to be known as “the endowment effect”: people demand more money
in exchange for an object that has been given to them than they would spend to
acquire the object in the first place. In psychologist Daniel Kahneman’s words,
“a good is worth more when it is considered as something that could be lost or
given up than when it is evaluated as a potential gain.”33 This aversion to
loss causes human beings to generally err on the side of maintaining the status
quo. It is also an important impediment to conflict resolution through
negotiation: for if each party values his opponent’s concessions as gains and
his own as losses, each is bound to perceive his sacrifice as being greater.34

Loss aversion has been studied with functional magnetic resonance imaging
(fMRI). If this bias were the result of negative feelings associated with
potential loss, we would expect brain regions known to govern negative emotion
to be involved. However, researchers have not found increased activity in any
areas of the brain as losses increase. Instead, those regions that represent
gains show decreasing activity as the size of the potential losses increases.
In fact, these brain structures themselves exhibit a pattern of “neural loss
aversion”: their activity decreases at a steeper rate in the face of potential
losses than they increase for potential gains.35

There are clearly cases in which such biases seem to produce moral
illusions—where a person’s view of right and wrong will depend on whether an
outcome is described in terms of gains or losses. Some of these illusions might
not be susceptible to full correction. As with many perceptual illusions, it
may be impossible to “see” two circumstances as morally equivalent, even while
“knowing” that they are. In such cases, it may be ethical to ignore how things
seem. Or it may be that the path we take to arrive at identical outcomes really
does matter to us—and, therefore, that losses and gains will remain
incommensurable.

Imagine, for instance, that you are empaneled as the member of a jury in a
civil trial and asked to determine how much a hospital should pay in damages to
the parents of children who received substandard care in their facility. There
are two scenarios to consider:



Couple A learned that their three-year-old daughter was inadvertently given a
neurotoxin by the hospital staff. Before being admitted, their daughter was a
musical prodigy with an IQ of 195. She has since lost all her intellectual
gifts. She can no longer play music with any facility and her IQ is now a
perfectly average 100.



Couple B learned that the hospital neglected to give their three-year-old
daughter, who has an IQ of 100, a perfectly safe and inexpensive genetic
enhancement that would have given her remarkable musical talent and nearly
doubled her IQ. Their daughter’s intelligence remains average, and she lacks
any noticeable musical gifts. The critical period for giving this enhancement
has passed.


Obviously the end result under either scenario is the same. But what if the
mental suffering associated with loss is simply bound to be greater than that
associated with forsaken gains? If so, it may be appropriate to take this
difference into account, even when we cannot give a rational explanation of why
it is worse to lose something than not to gain it. This is another source of
difficulty in the moral domain: unlike dilemmas in behavioral economics, it is
often difficult to establish the criteria by which two outcomes can be judged
equivalent.36 There is probably another principle at work in this example,
however: people tend to view sins of commission more harshly than sins of
omission. It is not clear how we should account for this bias either. But, once
again, to say that there are right answers to questions of how to maximize
human well-being is not to say that we will always be in a position to answer
such questions. There will be peaks and valleys on the moral landscape, and
movement between them is clearly possible, whether or not we always know which
way is up.

There are many other features of our subjectivity that have implications for
morality. For instance, people tend to evaluate an experience based on its peak
intensity (whether positive or negative) and the quality of its final moments.
In psychology, this is known as the “peak/end rule.” Testing this rule in a
clinical environment, one group found that patients undergoing colonoscopies
(in the days when this procedure was done without anesthetic) could have their
perception of suffering markedly reduced, and their likelihood of returning for
a follow-up exam increased, if their physician needlessly prolonged the
procedure at its lowest level of discomfort by leaving the colonoscope inserted
for a few extra minutes.37 The same principle seems to hold for aversive
sounds38 and for exposure to cold.39 Such findings suggest that, under certain
conditions, it is compassionate to prolong a person’s pain unnecessarily so as
to reduce his memory of suffering later on. Indeed, it might be unethical to do
otherwise. Needless to say, this is a profoundly counterintuitive result. But
this is precisely what is so important about science: it allows us to
investigate the world, and our place within it, in ways that get behind first
appearances. Why shouldn’t we do this with morality and human values generally?

