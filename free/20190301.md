So right now I am at a point where I am not sure what to write.

I want to write a little bit about linear algebra. So the first thing I want to
say is that it's important to understand the difference between a vector, and
it's coordinates.

In fact, in some vector spaces, it makes more sense to think only in terms of
some basis vectors, rather then in terms of the "construct itself" at all. So
for instance, instead of thinking about a polynomial as some function of some
input, if instead we think about a spanning set of polynomials, and think of
every polynomial as a linear combination of these members of the spanning set,
we can then gather a better understanding of what is happening.

Another important thing to remember in linear algebra is that a matrix, on it's
own, means nothing. A matrix of coordinates only makes sense if you have some
idea of what the basis is. In fact, every matrix could be thought of as some
manipulation on the basis vectors to the manipulation of those same basis
vectors on to an image space, and all of the vectors that span the domain
become well defined by the linearity of the mapping.

So in essence, a linear map is just a function from one space to another where
the way that the basis vectors behave in this map entirely defines the way that
every vector which spans that basis also behaves.

So now, the question becomes, how do we define the function. As we have said,
we can entirely define it as a matrix over some basis of v and some basis of w.

But the thing is, when we do this, we see that if we change our basis v and our
basis w, we will get a new linear map.

To make this more concrete, lets define our "Matrix" function a little more
concretely.

Matrix : Domain Basis -> Range Basis -> Map -> Matrix

Call the map F.
A vector in the domain basis will be called v_i where i is a number in the
dimension of v (called M), and a vector in the range basis will be called w_j,
j in dim W (called N)

Before we can define the matrix function, we must define

Column : Domain Basis -> Vector -> Matrix

Column is the matrix you get when you write out the vector v as a linear
combination of the basis. So to be explicit, we know that v can be written as

    v = SUM(a_i * v_i)

Column returns the Nx1 matrix X where X_i1 = a_i

To return to our definiton of Matrix now, the matrix we will get out will have
dimensions NxM. Call it A.

    A_ij = Column [w_j] (F v_i)

In words, the "i'th" column of A will be the value of our function F on the
i'th basis vector in the domain as a linear combination of the [w_j] basis.

So the thing to remember about this defenition of Matrix is that it depends on
the basis that we use. What linear algebra is about is understanding when two
matrices are congruent with another.

So for instance, suppose I have a matrix A, and a matrix B, each of which are
operators on some vector space V. Now, we have an operator, call it F_A, where,
for some given basis B_A, A = Matrix B_A B_A F_A. Likewise, say that for matrix
B there are these Basis and opertor such that B = Matrix B_B B_B F_B.

Now, we have some question that rise up out of this way of thinking about
things.

We want to ask, suppose we fix F in each case, so that F_A = F_B. Can we say
something like B_A = B_B -> A = B ? I would presume so, but it's certainly not
entirely trivial to say.

Likewise, if we say that B_A = B_B, can we say something like F_A = F_B -> A =
B? It's a similar question, but also not trivial to prove.

Before we get bogged down in any sort of mathematical way of proving this, let
us convince ourselves of what we should expect the result to be, so that we can
gain some further understanding of what we should expect.

So essentially what we are asking is what happens to an operator's matrix when
we change the basis?

So when we change a basis, what we are doing is taking an old basis, call in
[v_i] and changing it to a new one, call in [w_i] where w_i = SUM (T_ij * v_j).
In other words, since [v_i] is a basis, we can write any w_i in our new basis
as a linear combination of the vectors in the old basis. (We will return to T in
a bit.) (Now, it seems as though there are two equivilent ways of defining this
matrix T, the other being w_j = SUM (T_ij * v_i). This second way is inferior,
though, because we want to keep the ways that we use column matrices consitent.
We don't want to use row matrices.)

This makes since. How else are we going to define the new basis we would like
to use?

So suppose we have an operator, call it F, and we know that Matrix [v] [v] F is
A.

We want to ask ourselves, what is B = Matrix [w] [w] F, in terms of A and our
change of basis matrix?

To give an intuition about this, let's take a vector x and think about applying
matrix A to the vector x. To do this, we need to find the column representaiton
of x in terms of Basis A, then we multiply A and our column vector, and the
resulting column vector is the resulting vector expressed in terms of Basis A
Now, let's think about what the matrix B must look like. B takes our same
vector x, but requires that it be represented as a column vector of the Basis
B, once we have done this, we do matrix multiplication of B and this column
vector, and the resulting column vector is the resulting vector as a linear
combination in basis B.

So we should be able to find B by starting with a column vector in B,
converting that column vector to a column vector in A, applying A to that, and
then converting the result as a column vector in A to a column vector in B. So
in other words, if we know how to convert a column vector of one basis into a
column vector of another basis, we know how to convert matrices.

So let's work on some sort of "convert" function. The way convert will work is,
it will take in a column vector of some basis A and return a column vector of
basis B. Returning back to our "change of basis" question, we would like to be
able to define convert in terms of the matrix T. To remind ourselves, each
row of T represents the linear combination of vectors in our original basis
to get our basis vector for that row.

It turns out that performing matrix multiplication on T and a column vector in
basis A returns that same column vector on basis B.

But what basis are we using for T? The fact that we have made a matrix here
suggests that T is also something of a linear operator.

Let's not think too deeply on that question though, and resume our analysis.

So if we call the column vector a = (v, Basis A) to be the vector v as a linear
combination of the basis A, and likewise the column vector b = (v, basis B) to
be the linear combination of v on basis B, then we have the following.

1. Aa = a'  | By definition
2. Bb = b'  | By definition
3. Ta = b   | Change of basis
4. Ta' = b' | Change of basis for our image
5. T^-1b = a
5. BTa = b' | (3) substituted in to (2)

Bb = b'
B(Ta) = b'
TAT^-1b = Bb
B = TAT^-1

So something more that is required to define B in terms of A is that T must be
invertable. There is most definitely a proof that, if v_i and w_i are bases of
a space, then T is invertable.

In fact, we haven't formally defined T. Let's do that now.

    Converter : Basis -> Basis -> Matrix

What converter does is take in the first basis, then a second basis, and then
generates the matrix which converts a column vector in the first basis into a
column vector of the second basis.

It's pretty clear from the definition of Converter (which I have omitted in
detail, but could be filled in) that Converter V W is the inverse of Converter
W V. Our "T" that we have been using above can be defined using our converter
function directly.

So if we have a map on V, and it's matrix with respect to basis [v] is A,
then the matrix of that map with respect to basis [w] called B is given by

    B = TAT^-1

At this point, now, I want to define an equivelence class. I want to say
something like A ~ B iff. there exists a T such that B = TAT^-1.

Of course, now that we have done this, we want to ask ourselves questions about
the member of this equivilence class. Essentially, what we have done is stated
that every linear operator can be represented in any basis, and any of those
matrices are in the same equivilence class.

However, what we want to make sure didn't happen is, we don't want any
operators, call them T and U, where T != U, but Matrix [v] T = Matrix [w] U.

So in other words, could it be possible that a matrix A could represent one
operator with respect to one basis, but a totally different operator with
respect to a different basis?

We would like the answer to be no, but we have to ask ourselves in what way we
plan to go about showing this.

The first thing to do is to try and find a counter example. So in other words,
can we find a matrix A where A is the matrix of T in some basis, and also the
matrix of U in another, and T is not equal to U.

Since R is not equal to S, lets define some terms here.

Matrix R [v] we will call RV. Likewise, we have RW, SV, SW.

So RV != SV and RW != SW (because we have said that the operators are not
equal, their matrices in the same basis must be different). If we can find a RV
that is equal to SW (or SV = RW) then we have found our counter example.

We have to remind ourselves about converting matrices from one base to another.
So, let's call T the matrix which converts columns vectors in R to column
vectors in S.

What we have is RW = T RV T^-1. This is clear from the defintion of RW and RV. 
We have a set of four equations, actually. Let's write them out.

1. RW = T    RV T^-1
2. RV = T^-1 RW T
3. SW = T    SV T^-1
4. SV = T^-1 SW T

So we want to ask, under what conditions does RV = SW? Lets find RV and replace
it with SW and vice versa and see.

5. RW = T    SW T^-1
6. SW = T^-1 RW T
7. RV = T    SV T^-1
8. SV = T^-1 RV T

What we can see from this is that RW = T T SV T^-1 T^-1, which should raise
some eyebrows. This doesn't make intuitive sense. In order to show that this is
not a valid counterexample, what we have to do is show that, from these
equations, RV = SV and RU = SU.

9.  RW = T T SV T^-1 T^-1 (5 and 3)
10. SW = T^-1 T RV T^-1 T (6 and 1)
11. RV = T T^-1 SW T T^-1 (7 and 4)
12. SV = T^-1 T^-1 RW T T (8 and 2)

The middle two don't say anything new (just SW = RV which we already knew) but
the top and botton require further understanding.

    RW = T T T^-1 SW T T^-1 T^-1 (9 and 4)
    SV = T^-1 T^-1 T RV T^-1 T T (12 and 1)
    RW = T SW T^-1
    SV = T^-1 RV T
13. RW T = T SW
14. T SV = RV T

At this point, I have realized that I have forgotten some equations.

15. RV SW = RV T SV T^-1
16. RW SW = T RV SV T^-1



--

So I want to start over with todays entry.

I would like to talk about linear functions from one vector space to another.

A linear function is entirely defined by a set of lineraly independent vectors
that spans the domain of the function, a set of lineraly independent vectors
that spans the range of the function, and a matrix of values. If the dimension
of the domain is M, and the dimension of the range is N, then the matrix will
be an NxM matrix.

A linear map, defined in this way, will create a matrix which takes the
components of a vector in V with respect to the first basis, and transforms
those components to the components of the result of the linear map with respect
to the second basis.

So something that needs to be said. Two linear maps are equivelent not only
when their matrix and their basis sets are the same, but also when transforming
a matrix to another basis yeilds the other matrix in that basis.

So for instance, if we say that a linear map is something like an abstract
type, which is composed of a Matrix, A Domain Base, and a Range Base, and we
say that V, V', W, W' are bases and A and B are matrices, then

    Map V W A = Map V' W' B iff (matrix W' W I) B (matrix V V' I) = A

In other words, two linear maps are equal if transforming the bases of one map
to the other map transforms the matrix of that map to be the same as the matrix
of the other.

When looked at in this way, matrix and Map are defined mutually.

Map V W (matrix V W A) is equal to the linear map A

matrix V W (Map V W M) is equal to the matrix M

So now we want to talk about transformation of bases.

We used this fact in the last thing of talking, but I want to get it out now.

    matrix V W' A = (matrix W W' I) (matrix V W A)
    matrix V' W A = (matrix V W A) (matrix V' V I)

Suppose we define a vector. A basis is a list of vectors.
Now suppose we had a function that could take any vector, and write it as a
linear combination of our basis. This function would depend on the vector
space. We call this function "Column".

    column : [vector] -> vector -> [field]

where the property

    vector = sum (zipwith scalarMult (column Bs vector) Bs)

What this means is that

    matrix Vs Ws A = fmap (\v -> column (A v) Ws) Vs
